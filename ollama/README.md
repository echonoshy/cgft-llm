# å¤§æ¨¡å‹å¿«é€Ÿéƒ¨ç½²å·¥å…·Ollamaä»‹ç»

## ğŸŒ¿ åœ¨linuxä¸Šæ‰‹åŠ¨å®‰è£…ollama
Macå’ŒWindowså…·æœ‰æˆç†Ÿçš„ä¸€é”®å®‰è£…å·¥å…·ï¼ŒåŸºæœ¬éƒ½èƒ½å®‰è£…æˆåŠŸã€‚è¿™é‡Œæåˆ°å¦‚ä½•åœ¨linuxï¼ˆå†…ç½‘ï¼Œé…ç½®é—®é¢˜ï¼‰æ‰‹åŠ¨å®‰è£…linuxã€‚  
å½“ç„¶ä½ ä¹Ÿå¯ä»¥å°è¯•å®˜æ–¹ç»™å‡ºçš„ä¸€é”®è„šæœ¬ï¼š`curl -fsSL https://ollama.com/install.sh | sh`

### 1. å¯åŠ¨autodlä»£ç†(éå¿…éœ€ï¼‰
```
source /etc/network_turbo
```

### 2. æ‰‹åŠ¨ä¸‹è½½å®‰è£…åŒ…
ä¸‹è½½ollama linux:  
```
# æ‰‹åŠ¨ä»release ä¸­ä¸‹è½½amd64ç‰ˆæœ¬çš„å®‰è£…æ–‡ä»¶
https://github.com/ollama/ollama/releases

# æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œä¸‹è½½ï¼Œ æ³¨æ„ä¿®æ”¹ç‰ˆæœ¬å·ï¼ˆå¦‚v0.1.38)è‡³æœ€æ–°ç‰ˆæœ¬
curl -fsSL https://github.com/ollama/ollama/releases/download/v0.1.38/ollama-linux-amd64 -o /root/autodl-tmp/apps/ollama-linux-amd64

æˆ–è€…ï¼š
wget -O /root/autodl-tmp/apps/ollama-linux-amd64 https://github.com/ollama/ollama/releases/download/v0.1.38/ollama-linux-amd64
```

### 3. æŒ‚è½½è½¯è¿æ¥
```
sudo ln -s /root/autodl-tmp/apps/ollama-linux-amd64 /usr/local/bin/ollama
chmod +x /root/autodl-tmp/apps/ollama-linux-amd64
```
è¡¥å……ï¼š 

- **`/bin`**ï¼šåŸºæœ¬å‘½ä»¤ï¼Œå¿…é¡»åœ¨ç³»ç»Ÿå¼•å¯¼æ—¶å’Œå•ç”¨æˆ·æ¨¡å¼ä¸‹å¯ç”¨ã€‚ls, rm
- **`/usr/bin`**ï¼šæ ‡å‡†ç”¨æˆ·å‘½ä»¤ï¼Œé€šå¸¸ç”±ç³»ç»ŸåŒ…ç®¡ç†å™¨å®‰è£…ï¼Œä¾èµ–äº `/usr` æ–‡ä»¶ç³»ç»Ÿã€‚ sed, grep
- **`/usr/local/bin`**ï¼šæœ¬åœ°å®‰è£…çš„è½¯ä»¶å’Œè„šæœ¬ï¼Œä¸ä¾èµ–äºç³»ç»ŸåŒ…ç®¡ç†å™¨ã€‚

### 4. å¯åŠ¨æœåŠ¡
å¯åŠ¨æœåŠ¡ï¼š
```
ollama server 
```

## ğŸ§¢ å¦‚ä½•ä½¿ç”¨ollama
### 1. ä½¿ç”¨å¯¹è¯æ¨¡å¼
```
ollama run llama3   # æ‹‰å–æ¨¡å‹å¹¶æ‰§è¡Œ
ollama list         # æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
ollama rm llama3    # åˆ é™¤llama3æ¨¡å‹
ollama pull         # æ‹‰å–æ¨¡å‹ï¼Œå¹¶ä¸æ‰§è¡Œ
```
å®é™…å¯ç”¨çš„æ¨¡å‹ï¼Œæ¯”åˆ—å‡ºçš„[library](https://ollama.com/library)æ›´å¤šï¼Œæ¨èå»huggingfaceçœ‹è¯¥æ¨¡å‹çš„è¯´æ˜æ–‡ä»¶ï¼Œç¡®å®šæ˜¯å¦å·²ç»è¢«ollamaæ”¯æŒã€‚
å…¶ä»–çš„ollamaå‘½ä»¤ï¼Œç±»ä¼¼dockerï¼Œæ¯”è¾ƒç®€å•ï¼Œè‡ªå·±å°è¯•ä¸€ä¸‹ã€‚

### 2. å¦‚ä½•ä½¿ç”¨REST apiæ ¼å¼è°ƒç”¨
ä¸‹é¢åªå±•ç¤ºæœ€å¸¸ç”¨çš„apiç”¨æ³•ï¼Œæ›´å¤šå‚æ•°ä½¿ç”¨å»ºè®®é˜…è¯»å®˜æ–¹apiæ–‡æ¡£ï¼š https://github.com/ollama/ollama/blob/main/docs/api.md     
æ³¨ï¼šéæµå¼è¡¨ç¤ºä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰ç»“æœï¼Œæµå¼è¡¨ç¤ºæ¯æ¬¡è¿”å›ä¸€ä¸ªæ–°ç”Ÿæˆçš„tokenã€‚  


æµå¼è°ƒç”¨
```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?"
}'
```

éæµå¼è°ƒç”¨
```
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

å¸¦ä¸Šä¸‹æ–‡è°ƒç”¨ï¼ˆå¤šè½®å¯¹è¯ï¼‰
```
curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
    {
      "role": "user",
      "content": "why is the sky blue?"
    },
    {
      "role": "assistant",
      "content": "due to rayleigh scattering."
    },
    {
      "role": "user",
      "content": "how is that different than mie scattering?"
    }
  ],
  "stream": false
}'
```

### å¦‚ä½•ä»åŠ è½½æœ¬åœ°çš„æ¨¡å‹
ollamaæ”¯æŒçš„æ¨¡å‹æ ¼å¼ä¸ºggufï¼Œå¦‚æœä¸æ˜¯ggufæ ¼å¼éœ€è¦ä½¿ç”¨llama.cppå°†å…¶è½¬æˆggufã€‚   
âš ï¸æ³¨æ„ï¼šollamaå¹¶ä¸èƒ½å…¼å®¹æ‰€æœ‰çš„ggufæ–‡ä»¶ï¼

1. æ„å»ºModelfile
é™¤äº†å¿…é¡»åˆ¶å®šæ–‡ä»¶è·¯å¾„ï¼Œå…¶ä»–å‚æ•°éƒ½æ˜¯å¯é€‰é¡¹ã€‚

```
FROM PATH_TO_YOUR__GGUF_MODEL

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
```

2. åŠ è½½æ¨¡å‹
```
ollama create NAME -f ./Modelfile

# NAME: åœ¨ollamaä¸­æ˜¾ç¤ºçš„åç§°
# ./Modelfile: ç»å¯¹æˆ–è€…ç›¸å¯¹è·¯å¾„
```

3. ä½¿ç”¨æ¨¡å‹
```
ollama run NAME
```
