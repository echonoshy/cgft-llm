
# ã€å¤§æ¨¡å‹é‡åŒ–ã€‘ä½¿ç”¨llama.cppè¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²

ğŸ¥ è§†é¢‘æ•™ç¨‹
- [YouTube](https://youtu.be/2MYsfe0pc9A)
- [Bilibili](https://www.bilibili.com/video/BV1et421N7TK/)

## ğŸ“ 1. å‡†å¤‡æ¨¡å‹æ–‡ä»¶
- [shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)
- [shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit)

```bash
export HF_ENDPOINT=https://hf-mirror.com

huggingface-cli download shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit --local-dir /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF
```

## ğŸ”§ 2. å®‰è£…
```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

## ğŸ› ï¸ 3. ç¼–è¯‘

è¡¥å……ï¼š  
**CMake** æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„æ„å»ºç³»ç»Ÿç”Ÿæˆå·¥å…·ã€‚å®ƒçš„ä¸»è¦ä½œç”¨æ˜¯é€šè¿‡é…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ `CMakeLists.txt`ï¼‰ç”Ÿæˆé€‚åˆäºç›®æ ‡å¹³å°çš„æ„å»ºè„šæœ¬æˆ–æ–‡ä»¶

**Make** æ˜¯ä¸€ä¸ªæ„å»ºè‡ªåŠ¨åŒ–å·¥å…·ã€‚å®ƒé€šè¿‡è¯»å– Makefile æ¥æ‰§è¡Œç¼–è¯‘å’Œæ„å»ºè¿‡ç¨‹ã€‚

**g++/clang/MinGW** æ˜¯è´Ÿè´£å…·ä½“ç¼–è¯‘çš„ç¼–è¯‘å™¨ã€‚

**æ€»ç»“ï¼š**
- CMake ç”Ÿæˆ Makefileã€‚
- Make è¯»å– Makefile å¹¶è°ƒç”¨ g++ è¿›è¡Œç¼–è¯‘å’Œé“¾æ¥ã€‚
- g++ æ˜¯å®é™…æ‰§è¡Œç¼–è¯‘å’Œé“¾æ¥çš„ç¼–è¯‘å™¨ã€‚

### ğŸ–¥ï¸ CPU ç‰ˆæœ¬
```bash
cmake -B build_cpu
cmake --build build_cpu --config Release
```

### ğŸ–¥ï¸ CUDA ç‰ˆæœ¬
```bash
cmake -B build_cuda -DLLAMA_CUDA=ON
cmake --build build_cuda --config Release -j 12
```

## ğŸš€ 4. å…·ä½“ä½¿ç”¨

### ğŸ§© 4.1 ä¸»åŠŸèƒ½ main 
```bash
cd /root/code/llama.cpp/build_cuda/bin/

./main -m /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf \
    -n -1 \
    -ngl 256 \
    -t 12 \
    --color \
    -r "User:" \
    --in-prefix " " \
    -i \
    -p \
'User: ä½ å¥½
AI: ä½ å¥½å•Šï¼Œæˆ‘æ˜¯å…‰å±¿ï¼Œè¦èŠèŠå—?
User: å¥½å•Š!
AI: ä½ æƒ³èŠèŠä»€ä¹ˆè¯é¢˜å‘¢ï¼Ÿ
User:'
```

[main å‚æ•°ä»‹ç»](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)

### ğŸŒ 4.2 éƒ¨ç½²æœåŠ¡ server
```bash
cd ~/code/llama.cpp/build_cuda/bin

./server \
    -m /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf \
    --host "127.0.0.1" \
    --port 8080 \
    -c 2048 \
    -ngl 128 \
    --api-key "echo in the moon"
```

### ğŸ”§ 4.3 é‡åŒ–

æ··åˆç²¾åº¦é‡åŒ–ï¼š
1. fp16 -> int8 
2. fp16 -> fp16

ğŸ¤” æ€è€ƒï¼šå¦‚æœé‡‡æ ·æ··åˆç²¾åº¦é‡åŒ–æ—¶ï¼Œæœ‰çš„å±‚æ˜¯ fp16ï¼Œæœ‰çš„å±‚æ˜¯ int8ï¼Œè®¡ç®—æ—¶æ˜¯æ€æ ·çš„å‘¢ï¼Ÿ

1. å°† gguf æ ¼å¼è¿›è¡Œï¼ˆå†ï¼‰é‡åŒ–
```bash
cd ~/code/llama.cpp/build_cuda/bin
./quantize --allow-requantize /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v2_1.gguf /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q4_1-v1.gguf Q4_1
```

2. å°† safetensors æ ¼å¼è½¬æˆ gguf
```bash
python convert-hf-to-gguf.py /root/autodl-tmp/models/Llama3-8B-Chinese-Chat --outfile /root/autodl-tmp/models/Llama3-8B-Chinese-Chat-GGUF/Llama3-8B-Chinese-Chat-q8_0-v1.gguf --outtype q8_0
```

æ‰©å±•é˜…è¯»ï¼š  
[https://github.com/ggerganov/llama.cpp/pull/1684](https://github.com/ggerganov/llama.cpp/pull/1684)
